{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neo4j-field/call-transcripts-automation/blob/main/ui-notebook/ui.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KG + AI Customer Service Assistant\n",
    "\n",
    "This notebook replicates the Next.js UI for the Call Transcripts Automation project using **Gradio**.\n",
    "It connects to a Neo4j knowledge graph and uses Azure OpenAI to simulate customer calls,\n",
    "generate recommendations, and visualize process maps.\n",
    "\n",
    "**How to use:**\n",
    "1. Run all cells in order\n",
    "2. Fill in your credentials in Cell 2 (use Colab Secrets or a `.env` file)\n",
    "3. Click **New Call** to start a simulated customer conversation\n",
    "4. Type responses as the employee and click **Send**\n",
    "5. Use the sidebar for recommendations, suggested responses, and process maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio neo4j langchain-openai langchain-core openai pyvis python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# In Google Colab, store credentials in Secrets (key icon in left sidebar).\n",
    "# Locally, create a .env file in this directory.\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    for key in [\n",
    "        \"AZURE_OPENAI_API_KEY\", \"AZURE_OPENAI_ENDPOINT\",\n",
    "        \"NEO4J_URI\", \"NEO4J_USERNAME\", \"NEO4J_PASSWORD\", \"NEO4J_DATABASE\",\n",
    "        \"OPENAI_API_KEY\",\n",
    "    ]:\n",
    "        try:\n",
    "            os.environ[key] = userdata.get(key)\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"Loaded credentials from Colab Secrets\")\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"Loaded credentials from .env file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# --- Neo4j ---\n",
    "driver = GraphDatabase.driver(\n",
    "    os.environ[\"NEO4J_URI\"],\n",
    "    auth=(os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"]),\n",
    ")\n",
    "driver.verify_connectivity()\n",
    "DATABASE = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "print(f\"Connected to Neo4j at {os.environ['NEO4J_URI']}\")\n",
    "\n",
    "# --- Embedder ---\n",
    "embedder = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_version=\"2025-01-01-preview\",\n",
    "    dimensions=128,\n",
    ")\n",
    "\n",
    "# --- LLMs ---\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    temperature=0.8,\n",
    "    openai_api_version=\"2025-01-01-preview\",\n",
    ")\n",
    "\n",
    "llm_mini = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    openai_api_version=\"2025-01-01-preview\",\n",
    ")\n",
    "\n",
    "# Structured output for validation\n",
    "class ValidationResult(BaseModel):\n",
    "    validated: bool\n",
    "\n",
    "validation_llm = llm_mini.with_structured_output(ValidationResult)\n",
    "\n",
    "print(\"LLM and embedder initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "DEFAULT_K = 7\n",
    "COMMENT_EMBEDDINGS_INDEX_NAME = \"commentEmbeddings\"\n",
    "\n",
    "\n",
    "# ── 4a: sim_user ─────────────────────────────────────────────────────────────\n",
    "\n",
    "def sim_user(comment_id, messages):\n",
    "    \"\"\"Simulate a customer response.\n",
    "\n",
    "    Args:\n",
    "        comment_id: Current comment ID in the sample conversation, or None to start.\n",
    "        messages: List of dicts with 'role' and 'content' keys.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (comment_dict, new_comment_id).\n",
    "    \"\"\"\n",
    "    # Step 1: Validate / resolve comment ID\n",
    "    with driver.session(database=DATABASE) as session:\n",
    "        if not comment_id:\n",
    "            result = session.run(\n",
    "                \"MATCH (c:Call) \"\n",
    "                \"WITH c ORDER BY rand() LIMIT 1 \"\n",
    "                \"MATCH (c)-[:FIRST]->(comm) \"\n",
    "                \"RETURN comm\"\n",
    "            )\n",
    "            record = result.single()\n",
    "            comment_id = record[\"comm\"][\"id\"]\n",
    "        elif messages:\n",
    "            validation_messages = [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"Review the messages and determine if the user has addressed \"\n",
    "                    \"the most recent comment from their counterparty. \"\n",
    "                    \"View the final assistant message and then look if the final \"\n",
    "                    \"user message has adequately addressed it. \"\n",
    "                    \"Note that in some situations, the employee will need to ask \"\n",
    "                    \"for information — this can be considered a temporary resolution \"\n",
    "                    \"and the conversation can progress. \"\n",
    "                    \"If it has, return validated: true. Otherwise return validated: false.\",\n",
    "                ),\n",
    "            ] + [(m[\"role\"], m[\"content\"]) for m in messages]\n",
    "\n",
    "            result = validation_llm.invoke(validation_messages)\n",
    "\n",
    "            if result.validated:\n",
    "                query_result = session.run(\n",
    "                    \"OPTIONAL MATCH (comm:Comment {id: $commentId})\"\n",
    "                    \"-[:NEXT]->()-[:NEXT]->(nextCustomerComm:Comment) \"\n",
    "                    \"RETURN nextCustomerComm AS nextComm\",\n",
    "                    commentId=comment_id,\n",
    "                )\n",
    "                record = query_result.single()\n",
    "                next_comm = record[\"nextComm\"]\n",
    "                if next_comm:\n",
    "                    comment_id = next_comm[\"id\"]\n",
    "\n",
    "    # Step 2: Get comment sample\n",
    "    with driver.session(database=DATABASE) as session:\n",
    "        result = session.run(\n",
    "            \"MATCH (comm:Comment {id: $commentId}) \"\n",
    "            \"MATCH (comm)-[:OBSERVED_STATE]->(:Observation:State)\"\n",
    "            \"-[:IS_PROCESS_ELEMENT]->(:ProcessElement:State)\"\n",
    "            \"<-[:IS_PROCESS_ELEMENT]-(:Observation:State)\"\n",
    "            \"<-[:OBSERVED_STATE]-(similarComment:Comment) \"\n",
    "            \"WITH comm, similarComment \"\n",
    "            \"ORDER BY vector.similarity.cosine(comm.embedding, similarComment.embedding) DESC \"\n",
    "            \"LIMIT toInteger($k) \"\n",
    "            \"RETURN collect(similarComment.content) AS commentSample\",\n",
    "            commentId=comment_id,\n",
    "            k=DEFAULT_K,\n",
    "        )\n",
    "        record = result.single()\n",
    "        comment_sample = record[\"commentSample\"] if record else []\n",
    "\n",
    "    # Step 3: Generate customer message\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are simulating a customer engaged with a customer service rep.\\n\"\n",
    "            \"You can see the comments up to this point in the message history, if any.\\n\"\n",
    "            \"From these, you can get a sense for the personality you're impersonating \"\n",
    "            \"and the flow of the conversation.\\n\"\n",
    "            \"You should produce the next message in the conversation.\\n\"\n",
    "            \"Make a comment that makes sense in the context of the conversation.\\n\"\n",
    "            \"Additionally, see below a list of sample comments.\\n\"\n",
    "            \"These are comments that are similar in intent to the comment you should \"\n",
    "            \"write next.\\n\\n\"\n",
    "            \"Comment Sample:\\n{comment_sample}\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm\n",
    "    formatted_messages = [(m[\"role\"], m[\"content\"]) for m in messages] if messages else []\n",
    "\n",
    "    response = chain.invoke({\n",
    "        \"messages\": formatted_messages,\n",
    "        \"comment_sample\": \"\\n\".join(comment_sample),\n",
    "    })\n",
    "\n",
    "    return {\"role\": \"assistant\", \"content\": response.content}, comment_id\n",
    "\n",
    "\n",
    "# ── 4b: sidebar_recommendation ───────────────────────────────────────────────\n",
    "\n",
    "def sidebar_recommendation(comments):\n",
    "    \"\"\"Generate a sidebar recommendation for the employee.\n",
    "\n",
    "    Args:\n",
    "        comments: List of dicts with 'role' and 'content'.\n",
    "\n",
    "    Returns:\n",
    "        Recommendation text.\n",
    "    \"\"\"\n",
    "    if not comments:\n",
    "        return \"*No conversation yet.*\"\n",
    "\n",
    "    # Step 1: Embed last comment\n",
    "    embedding = embedder.embed_documents([comments[-1][\"content\"]])[0]\n",
    "\n",
    "    # Step 2: Retrieve action paths\n",
    "    with driver.session(database=DATABASE) as session:\n",
    "        result = session.run(\n",
    "            \"CALL db.index.vector.queryNodes($commentEmbeddings, $k, $embedding) \"\n",
    "            \"YIELD node AS similarComment \"\n",
    "            \"MATCH (similarComment)-[:OBSERVED_STATE]->()-[:IS_PROCESS_ELEMENT]->(pe) \"\n",
    "            \"MATCH (pe)-[r:ACTION_SELECTION]->(action) \"\n",
    "            \"WITH r.probability AS probability, action \"\n",
    "            \"ORDER BY probability DESC \"\n",
    "            \"LIMIT 1 \"\n",
    "            \"MATCH (res:ProcessElement:Resolution) \"\n",
    "            \"MATCH p = shortestPath((action)-[:TRANSITION|ACTION_SELECTION|PROCESS_END*]->(res)) \"\n",
    "            \"RETURN \"\n",
    "            \"  reduce(prob = probability, rel IN relationships(p) | prob * rel.probability) AS historicalProbability, \"\n",
    "            \"  [node IN nodes(p) | node.name] AS path \"\n",
    "            \"ORDER BY historicalProbability DESC \"\n",
    "            \"LIMIT 5\",\n",
    "            embedding=embedding,\n",
    "            k=DEFAULT_K,\n",
    "            commentEmbeddings=COMMENT_EMBEDDINGS_INDEX_NAME,\n",
    "        )\n",
    "        paths = [\n",
    "            {\"path\": record[\"path\"], \"probability\": record[\"historicalProbability\"]}\n",
    "            for record in result\n",
    "        ]\n",
    "\n",
    "    # Step 3: Generate recommendation\n",
    "    comments_text = \"\\n\".join(\n",
    "        f\"{'Employee' if c['role'] == 'user' else 'Customer'}: {c['content']}\"\n",
    "        for c in comments[:-1]\n",
    "    )\n",
    "    latest = f\"{'Employee' if comments[-1]['role'] == 'user' else 'Customer'}: {comments[-1]['content']}\"\n",
    "    if comments_text:\n",
    "        comments_text += f\"\\n\\nLatest comment:\\n{latest}\"\n",
    "    else:\n",
    "        comments_text = latest\n",
    "\n",
    "    paths_text = \"\\n\".join(\", \".join(p[\"path\"]) for p in paths)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a conversational agent that is designed to help telecom \"\n",
    "            \"customer service representatives talk to customers.\\n\"\n",
    "            \"Your job is to recommend the next action to the representative.\\n\"\n",
    "            \"You will see the list of comments up to this point as well as your \"\n",
    "            \"own suggestion history to the rep.\\n\"\n",
    "            \"You will also get a list of recommended action paths the rep could \"\n",
    "            \"follow, ordered by historical probability.\\n\"\n",
    "            \"Please consider these paths, consider the sentiment and context, \"\n",
    "            \"and provide a recommendation.\\n\"\n",
    "            \"Put the recommendation into your own words and also advise on tone \"\n",
    "            \"and approach.\\n\"\n",
    "            \"Only rely on the context you've been provided — don't make up any \"\n",
    "            \"new information.\\n\"\n",
    "            \"Reply in 2 sentences or less. Do not recommend language. Just give advice.\\n\\n\"\n",
    "            \"Comments:\\n{comments}\\n\\nAction paths:\\n{paths}\",\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"comments\": comments_text, \"paths\": paths_text})\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# ── 4c: suggested_response ────────────────────────────────────────────────────\n",
    "\n",
    "def suggested_response(comments):\n",
    "    \"\"\"Generate one suggested response for the employee.\n",
    "\n",
    "    Args:\n",
    "        comments: List of dicts with 'role' and 'content'.\n",
    "\n",
    "    Returns:\n",
    "        Suggested response text.\n",
    "    \"\"\"\n",
    "    if not comments:\n",
    "        return \"\"\n",
    "\n",
    "    # Step 1: Embed last comment\n",
    "    embedding = embedder.embed_documents([comments[-1][\"content\"]])[0]\n",
    "\n",
    "    # Step 2: Retrieve action and resolutions\n",
    "    with driver.session(database=DATABASE) as session:\n",
    "        result = session.run(\n",
    "            \"CALL db.index.vector.queryNodes($commentEmbeddings, $k, $embedding) \"\n",
    "            \"YIELD node AS similarComment \"\n",
    "            \"MATCH (similarComment)-[:OBSERVED_STATE]->()-[:IS_PROCESS_ELEMENT]->(pe) \"\n",
    "            \"MATCH (pe)-[r:ACTION_SELECTION]->(action) \"\n",
    "            \"WITH r.probability AS probability, action \"\n",
    "            \"ORDER BY probability DESC \"\n",
    "            \"LIMIT 1 \"\n",
    "            \"MATCH (res:ProcessElement:Resolution) \"\n",
    "            \"MATCH p = shortestPath((action)-[:TRANSITION|ACTION_SELECTION|PROCESS_END*]->(res)) \"\n",
    "            \"WITH action, \"\n",
    "            \"  reduce(prob = probability, rel IN relationships(p) | prob * rel.probability) AS historicalProbability, \"\n",
    "            \"  [node IN nodes(p) | node] AS path \"\n",
    "            \"ORDER BY historicalProbability DESC \"\n",
    "            \"LIMIT 3 \"\n",
    "            \"RETURN action, collect(path[-1]) AS resolutions\",\n",
    "            embedding=embedding,\n",
    "            k=DEFAULT_K,\n",
    "            commentEmbeddings=COMMENT_EMBEDDINGS_INDEX_NAME,\n",
    "        )\n",
    "        record = result.single()\n",
    "        if not record:\n",
    "            return \"Unable to generate suggestion — no matching data found.\"\n",
    "        action = record[\"action\"]\n",
    "        resolutions = record[\"resolutions\"]\n",
    "\n",
    "    # Step 3: Get action samples\n",
    "    with driver.session(database=DATABASE) as session:\n",
    "        result = session.run(\n",
    "            \"MATCH (action:ProcessElement:Action {id: $actionId}) \"\n",
    "            \"WITH action \"\n",
    "            \"UNWIND $resolutionIds AS resolutionId \"\n",
    "            \"MATCH (resolution:ProcessElement:Resolution {id: resolutionId}) \"\n",
    "            \"MATCH (action)<-[:IS_PROCESS_ELEMENT]-(:Observation)\"\n",
    "            \"<-[:OBSERVED_ACTION]-(comm:Comment)\"\n",
    "            \"<-[:NEXT*]-()<-[:FIRST]-(:Call)\"\n",
    "            \"-[:OBSERVED_RESOLUTION]->(resolution) \"\n",
    "            \"WITH resolution, comm \"\n",
    "            \"ORDER BY rand() \"\n",
    "            \"WITH resolution, collect(comm.content) AS contents \"\n",
    "            \"RETURN resolution, contents[..2] AS contents\",\n",
    "            actionId=action[\"id\"],\n",
    "            resolutionIds=[r[\"id\"] for r in resolutions],\n",
    "        )\n",
    "        contents = [\n",
    "            {\"resolution\": record[\"resolution\"], \"contents\": record[\"contents\"]}\n",
    "            for record in result\n",
    "        ]\n",
    "\n",
    "    # Step 4: Generate\n",
    "    conversation_text = \"\\n\".join(\n",
    "        f\"{'Employee' if c['role'] == 'user' else 'Customer'}: {c['content']}\"\n",
    "        for c in comments[:-1]\n",
    "    )\n",
    "    latest = f\"{'Employee' if comments[-1]['role'] == 'user' else 'Customer'}: {comments[-1]['content']}\"\n",
    "    if conversation_text:\n",
    "        conversation_text += f\"\\n\\nLatest comment:\\n{latest}\"\n",
    "    else:\n",
    "        conversation_text = latest\n",
    "\n",
    "    suggestions_text = \"\\n\\n\".join(\n",
    "        f\"Outcome: {item['resolution']['name']}\\n\"\n",
    "        + \"\\n\".join(f\"Comment: {c}\" for c in item[\"contents\"])\n",
    "        for item in contents\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a conversational agent that is designed to help telecom \"\n",
    "            \"customer service representatives talk to customers.\\n\"\n",
    "            \"Your job is to suggest a response in the conversation based on the \"\n",
    "            \"context provided.\\n\"\n",
    "            \"This context includes both the conversation history as well as a \"\n",
    "            \"list of comments made by the rep in similar situations.\\n\"\n",
    "            \"Your response should ideally be 2-3 sentences long and should be \"\n",
    "            \"helpful and informative, but if it needs to be longer be as concise \"\n",
    "            \"as possible.\\n\"\n",
    "            \"Respect the tone and sentiment of the call and respect the \"\n",
    "            \"disposition of the customer.\\n\"\n",
    "            \"We want to be as helpful as possible to the customer and also have \"\n",
    "            \"an eye towards positive outcomes for us.\\n\"\n",
    "            \"The example comments are organized by projected outcome, so decide \"\n",
    "            \"what the best likely outcome is and respond accordingly.\\n\\n\"\n",
    "            \"Conversation history:\\n{conversation}\\n\\n{suggestions}\",\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\n",
    "        \"conversation\": conversation_text,\n",
    "        \"suggestions\": suggestions_text,\n",
    "    })\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# ── 4d: get_process_map ──────────────────────────────────────────────────────\n",
    "\n",
    "def get_process_map(comment_content):\n",
    "    \"\"\"Get process-map nodes and relationships for a comment.\n",
    "\n",
    "    Args:\n",
    "        comment_content: The text of the comment to analyse.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (nodes_list, rels_list).\n",
    "    \"\"\"\n",
    "    embedding = embedder.embed_documents([comment_content])[0]\n",
    "\n",
    "    with driver.session(database=DATABASE) as session:\n",
    "        result = session.run(\n",
    "            \"CALL db.index.vector.queryNodes($commentEmbeddings, $k, $embedding) \"\n",
    "            \"YIELD node AS similarComment \"\n",
    "            \"MATCH (similarComment)-[:OBSERVED_STATE]->()-[:IS_PROCESS_ELEMENT]->(pe) \"\n",
    "            \"MATCH (pe)-[r:ACTION_SELECTION]->(action) \"\n",
    "            \"WITH r.probability AS probability, action \"\n",
    "            \"ORDER BY probability DESC \"\n",
    "            \"LIMIT 1 \"\n",
    "            \"MATCH (res:ProcessElement:Resolution) \"\n",
    "            \"MATCH p = shortestPath((action)-[:TRANSITION|ACTION_SELECTION|PROCESS_END*]->(res)) \"\n",
    "            \"WITH nodes(p) AS nodes, relationships(p) AS rels \"\n",
    "            \"LIMIT 5 \"\n",
    "            \"WITH collect(nodes) AS allNodes, collect(rels) AS allRels \"\n",
    "            \"WITH apoc.coll.toSet(apoc.coll.flatten(allNodes)) AS nodes, \"\n",
    "            \"     apoc.coll.toSet(apoc.coll.flatten(allRels)) AS rels \"\n",
    "            \"RETURN \"\n",
    "            \"  [n IN nodes | { \"\n",
    "            \"    id: n.id + '', \"\n",
    "            \"    description: n.description, \"\n",
    "            \"    name: n.name, \"\n",
    "            \"    label: [lbl IN labels(n) WHERE lbl <> 'ProcessElement' | lbl][0] \"\n",
    "            \"  }] AS nodes, \"\n",
    "            \"  [r IN rels | { \"\n",
    "            \"    id: elementId(r), \"\n",
    "            \"    from: startNode(r).id + '', \"\n",
    "            \"    to: endNode(r).id + '', \"\n",
    "            \"    probability: r.probability, \"\n",
    "            \"    type: type(r) \"\n",
    "            \"  }] AS rels\",\n",
    "            embedding=embedding,\n",
    "            k=DEFAULT_K,\n",
    "            commentEmbeddings=COMMENT_EMBEDDINGS_INDEX_NAME,\n",
    "        )\n",
    "        record = result.single()\n",
    "        if not record:\n",
    "            return [], []\n",
    "        return record[\"nodes\"], record[\"rels\"]\n",
    "\n",
    "\n",
    "print(\"Agent functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import base64\n",
    "\n",
    "\n",
    "def render_process_map(nodes, rels):\n",
    "    \"\"\"Render process-map nodes and relationships as an interactive HTML graph.\"\"\"\n",
    "    if not nodes:\n",
    "        return \"<p style='text-align:center;color:#888;'>No process map data available.</p>\"\n",
    "\n",
    "    net = Network(\n",
    "        height=\"500px\",\n",
    "        width=\"100%\",\n",
    "        directed=True,\n",
    "        notebook=True,\n",
    "        cdn_resources=\"remote\",\n",
    "    )\n",
    "    net.barnes_hut(gravity=-3000, central_gravity=0.3, spring_length=200)\n",
    "\n",
    "    color_map = {\"State\": \"#93c5fd\", \"Action\": \"#6ee7b7\", \"Resolution\": \"#fca5a5\"}\n",
    "\n",
    "    for node in nodes:\n",
    "        color = color_map.get(node.get(\"label\"), \"#d4d4d8\")\n",
    "        net.add_node(\n",
    "            node[\"id\"],\n",
    "            label=node.get(\"name\", node[\"id\"]),\n",
    "            title=node.get(\"description\") or node.get(\"name\", \"\"),\n",
    "            color=color,\n",
    "            shape=\"dot\",\n",
    "            size=25,\n",
    "            font={\"size\": 14},\n",
    "        )\n",
    "\n",
    "    for rel in rels:\n",
    "        label = rel.get(\"type\", \"\")\n",
    "        prob = rel.get(\"probability\")\n",
    "        if prob is not None:\n",
    "            try:\n",
    "                label += f\" ({float(prob):.0%})\"\n",
    "            except (TypeError, ValueError):\n",
    "                pass\n",
    "        net.add_edge(\n",
    "            rel[\"from\"],\n",
    "            rel[\"to\"],\n",
    "            title=label,\n",
    "            label=label,\n",
    "            font={\"size\": 10},\n",
    "        )\n",
    "\n",
    "    html = net.generate_html()\n",
    "\n",
    "    # Inject a legend into the HTML\n",
    "    legend = (\n",
    "        '<div style=\"position:absolute;top:10px;left:10px;background:white;'\n",
    "        \"padding:8px;border-radius:8px;box-shadow:0 2px 4px rgba(0,0,0,0.1);\"\n",
    "        'z-index:100;font-family:sans-serif;font-size:12px;\">'\n",
    "        '<div style=\"display:flex;align-items:center;gap:6px;margin-bottom:4px;\">'\n",
    "        '<div style=\"width:14px;height:14px;border-radius:50%;background:#93c5fd;\"></div>State</div>'\n",
    "        '<div style=\"display:flex;align-items:center;gap:6px;margin-bottom:4px;\">'\n",
    "        '<div style=\"width:14px;height:14px;border-radius:50%;background:#6ee7b7;\"></div>Action</div>'\n",
    "        '<div style=\"display:flex;align-items:center;gap:6px;\">'\n",
    "        '<div style=\"width:14px;height:14px;border-radius:50%;background:#fca5a5;\"></div>Resolution</div>'\n",
    "        \"</div>\"\n",
    "    )\n",
    "    html = html.replace(\"<body>\", f\"<body>{legend}\")\n",
    "\n",
    "    b64 = base64.b64encode(html.encode()).decode()\n",
    "    return f'<iframe src=\"data:text/html;base64,{b64}\" width=\"100%\" height=\"550px\" frameborder=\"0\"></iframe>'\n",
    "\n",
    "\n",
    "print(\"Process map renderer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "# ── Event handlers ────────────────────────────────────────────────────────────\n",
    "\n",
    "def handle_new_call():\n",
    "    \"\"\"Start a new simulated customer call.\"\"\"\n",
    "    try:\n",
    "        comment, comment_id = sim_user(None, [])\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            [{\"role\": \"assistant\", \"content\": f\"[Error starting call: {e}]\"}],\n",
    "            f\"*Error: {e}*\",\n",
    "            None,\n",
    "            [],\n",
    "            \"\",\n",
    "            \"\", \"\", \"\",\n",
    "            \"\",\n",
    "        )\n",
    "\n",
    "    messages = [comment]\n",
    "    chat_history = [{\"role\": \"assistant\", \"content\": comment[\"content\"]}]\n",
    "\n",
    "    try:\n",
    "        rec = sidebar_recommendation(messages)\n",
    "    except Exception as e:\n",
    "        rec = f\"*Error generating recommendation: {e}*\"\n",
    "\n",
    "    return (\n",
    "        chat_history,\n",
    "        f\"**Recommendation:**\\n\\n{rec}\",\n",
    "        comment_id,\n",
    "        messages,\n",
    "        \"\",        # clear input\n",
    "        \"\", \"\", \"\",  # clear suggestions\n",
    "        \"\",        # clear process map\n",
    "    )\n",
    "\n",
    "\n",
    "def handle_send(user_msg, chat_history, comment_id, messages):\n",
    "    \"\"\"Send a user (employee) message and get a customer response.\"\"\"\n",
    "    if not user_msg or not user_msg.strip():\n",
    "        return (\n",
    "            chat_history or [],\n",
    "            \"*Enter a message first.*\",\n",
    "            comment_id,\n",
    "            messages or [],\n",
    "            \"\",\n",
    "        )\n",
    "\n",
    "    messages = list(messages or []) + [{\"role\": \"user\", \"content\": user_msg}]\n",
    "    chat_history = list(chat_history or []) + [{\"role\": \"user\", \"content\": user_msg}]\n",
    "\n",
    "    try:\n",
    "        comment, new_comment_id = sim_user(comment_id, messages)\n",
    "        messages = messages + [{\"role\": \"assistant\", \"content\": comment[\"content\"]}]\n",
    "        chat_history = chat_history + [\n",
    "            {\"role\": \"assistant\", \"content\": comment[\"content\"]}\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        chat_history = chat_history + [\n",
    "            {\"role\": \"assistant\", \"content\": f\"[Error: {e}]\"}\n",
    "        ]\n",
    "        new_comment_id = comment_id\n",
    "\n",
    "    try:\n",
    "        rec = sidebar_recommendation(messages)\n",
    "    except Exception as e:\n",
    "        rec = f\"*Error: {e}*\"\n",
    "\n",
    "    return (\n",
    "        chat_history,\n",
    "        f\"**Recommendation:**\\n\\n{rec}\",\n",
    "        new_comment_id,\n",
    "        messages,\n",
    "        \"\",\n",
    "    )\n",
    "\n",
    "\n",
    "def handle_use_suggestion(suggestion, chat_history, comment_id, messages):\n",
    "    \"\"\"Use a suggested response as the employee message.\"\"\"\n",
    "    if not suggestion or not suggestion.strip():\n",
    "        return (\n",
    "            chat_history or [],\n",
    "            \"*No suggestion to use.*\",\n",
    "            comment_id,\n",
    "            messages or [],\n",
    "            \"\",\n",
    "        )\n",
    "    return handle_send(suggestion, chat_history, comment_id, messages)\n",
    "\n",
    "\n",
    "def handle_generate_suggestions(messages):\n",
    "    \"\"\"Generate 3 suggested responses.\"\"\"\n",
    "    if not messages:\n",
    "        return \"No conversation yet.\", \"No conversation yet.\", \"No conversation yet.\"\n",
    "    results = []\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            results.append(suggested_response(messages))\n",
    "        except Exception as e:\n",
    "            results.append(f\"Error: {e}\")\n",
    "    return results[0], results[1], results[2]\n",
    "\n",
    "\n",
    "def handle_refresh_map(messages):\n",
    "    \"\"\"Refresh the process map based on the latest comment.\"\"\"\n",
    "    if not messages:\n",
    "        return \"<p style='text-align:center;color:#888;'>Start a conversation first.</p>\"\n",
    "    try:\n",
    "        nodes, rels = get_process_map(messages[-1][\"content\"])\n",
    "        return render_process_map(nodes, rels)\n",
    "    except Exception as e:\n",
    "        return f\"<p style='color:red;'>Error generating process map: {e}</p>\"\n",
    "\n",
    "\n",
    "# ── Build the Gradio UI ──────────────────────────────────────────────────────\n",
    "\n",
    "with gr.Blocks(title=\"KG + AI Assistant\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# KG + AI Customer Service Assistant\")\n",
    "\n",
    "    # State\n",
    "    comment_id_state = gr.State(value=None)\n",
    "    messages_state = gr.State(value=[])\n",
    "\n",
    "    with gr.Row():\n",
    "        # ---- Left column: Chat ----\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(\n",
    "                type=\"messages\",\n",
    "                height=500,\n",
    "                label=\"Conversation  (You = Employee, AI = Customer)\",\n",
    "            )\n",
    "            msg_input = gr.Textbox(\n",
    "                placeholder=\"Type your response as the employee...\",\n",
    "                label=\"Your Message\",\n",
    "                lines=2,\n",
    "            )\n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "                new_call_btn = gr.Button(\"New Call\", variant=\"secondary\")\n",
    "\n",
    "        # ---- Right column: Sidebar ----\n",
    "        with gr.Column(scale=1):\n",
    "            recommendation = gr.Markdown(\n",
    "                value=\"*Click **New Call** to start a conversation.*\",\n",
    "                label=\"Recommendation\",\n",
    "            )\n",
    "\n",
    "            with gr.Accordion(\"Details\", open=True):\n",
    "                with gr.Tab(\"Suggested Responses\"):\n",
    "                    gen_suggestions_btn = gr.Button(\n",
    "                        \"Generate Suggestions\", variant=\"primary\"\n",
    "                    )\n",
    "                    suggestion_1 = gr.Textbox(\n",
    "                        label=\"Suggestion 1\", interactive=False, lines=3\n",
    "                    )\n",
    "                    use_1_btn = gr.Button(\"Use Response 1\", size=\"sm\")\n",
    "                    suggestion_2 = gr.Textbox(\n",
    "                        label=\"Suggestion 2\", interactive=False, lines=3\n",
    "                    )\n",
    "                    use_2_btn = gr.Button(\"Use Response 2\", size=\"sm\")\n",
    "                    suggestion_3 = gr.Textbox(\n",
    "                        label=\"Suggestion 3\", interactive=False, lines=3\n",
    "                    )\n",
    "                    use_3_btn = gr.Button(\"Use Response 3\", size=\"sm\")\n",
    "\n",
    "                with gr.Tab(\"Process Map\"):\n",
    "                    refresh_map_btn = gr.Button(\n",
    "                        \"Refresh Process Map\", variant=\"primary\"\n",
    "                    )\n",
    "                    process_map_html = gr.HTML(\n",
    "                        value=\"<p style='text-align:center;color:#888;'>\"\n",
    "                        \"Click <b>Refresh</b> after starting a conversation.</p>\"\n",
    "                    )\n",
    "\n",
    "    # ---- Event wiring ----\n",
    "    new_call_outputs = [\n",
    "        chatbot, recommendation, comment_id_state, messages_state,\n",
    "        msg_input, suggestion_1, suggestion_2, suggestion_3, process_map_html,\n",
    "    ]\n",
    "    new_call_btn.click(fn=handle_new_call, inputs=[], outputs=new_call_outputs)\n",
    "\n",
    "    send_outputs = [\n",
    "        chatbot, recommendation, comment_id_state, messages_state, msg_input,\n",
    "    ]\n",
    "    send_btn.click(\n",
    "        fn=handle_send,\n",
    "        inputs=[msg_input, chatbot, comment_id_state, messages_state],\n",
    "        outputs=send_outputs,\n",
    "    )\n",
    "    msg_input.submit(\n",
    "        fn=handle_send,\n",
    "        inputs=[msg_input, chatbot, comment_id_state, messages_state],\n",
    "        outputs=send_outputs,\n",
    "    )\n",
    "\n",
    "    for btn, sug in [\n",
    "        (use_1_btn, suggestion_1),\n",
    "        (use_2_btn, suggestion_2),\n",
    "        (use_3_btn, suggestion_3),\n",
    "    ]:\n",
    "        btn.click(\n",
    "            fn=handle_use_suggestion,\n",
    "            inputs=[sug, chatbot, comment_id_state, messages_state],\n",
    "            outputs=send_outputs,\n",
    "        )\n",
    "\n",
    "    gen_suggestions_btn.click(\n",
    "        fn=handle_generate_suggestions,\n",
    "        inputs=[messages_state],\n",
    "        outputs=[suggestion_1, suggestion_2, suggestion_3],\n",
    "    )\n",
    "\n",
    "    refresh_map_btn.click(\n",
    "        fn=handle_refresh_map,\n",
    "        inputs=[messages_state],\n",
    "        outputs=[process_map_html],\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
